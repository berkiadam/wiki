

# Perceptron, mesters√©ges neuron

A neuron egy matematikai f√ºggv√©ny, amely n darab bemenetet k√©pez le egyetlen kimenetre.

A form√°lis egyenlet
Egyetlen neuron m≈±k√∂d√©s√©t az al√°bbi kompoz√≠ci√≥s f√ºggv√©ny √≠rja le:

$$
y = \sigma \left( \sum_{i=1}^{n} w_i x_i + b \right)
$$

Vagy t√∂m√∂rebb, vektoros jel√∂l√©ssel (ahol a pont a skal√°ris szorz√°st jel√∂li):

$$
y=œÉ(w‚ãÖx+b)
$$


Ahol: 


* $x$: (Bemeneti vektor): $x=[x_1, x_2, ...x_n]$. Ez a be√©rkez≈ë adatok vektora (p√©ld√°ul a lak√°s m√©rete, szob√°k sz√°ma). 
* $w$:(S√∫lyvektor - Weights): $w=[w_1, w_2, ...w_n]$ minden bemenethez tartozik egy s√∫ly, ami azt mutatja, mennyire fontos az adott bemenet a d√∂nt√©s szempontj√°b√≥l. A tanul√°s sor√°n ezeket az √©rt√©keket "√°ll√≠tgatjuk".
* $b$: (Eltol√°s - Bias): Egy skal√°r √©rt√©k, amely lehet≈ëv√© teszi, hogy a neuron line√°ris f√ºggv√©ny√©t eltoljuk a nullapontt√≥l. Ahogy a dokumentumod √≠rta: ez a korrekci√≥s sz√°m, ami "helyre teszi" az egyenest. Ugyan az eltol√°st az aktiv√°ci√≥s f√ºggv√©ny el≈ëtt alkalmazzuk, m√©g is nagy hat√°sa van az aktiv√°ci√≥ra. 
* $‚àë$: vagy ‚ãÖ (Line√°ris kombin√°ci√≥): A bemenetek √©s s√∫lyok szorzat√∂sszege. Ez a r√©sz felel meg a line√°ris regresszi√≥nak
* $œÉ$: (Aktiv√°ci√≥s f√ºggv√©ny): Ez a nem-line√°ris komponens (pl. Sigmoid, ReLU, Tanh). Ez d√∂nti el, hogy a neuron "t√ºzeljen-e", illetve milyen √©rt√©ket adjon tov√°bb. 
* $y$: (Kimenet): A neuron v√©gs≈ë v√°lasza.



![](docs/image-2025-12-14-19-52-13.png)


√çgy foglaln√°m √∂ssze egy sz√©p, kerek mondatban a neuron teljes m≈±k√∂d√©s√©t:

*"A neuron el≈ësz√∂r kisz√°m√≠tja a be√©rkez≈ë inform√°ci√≥k s√∫lyozott √∂sszeg√©t, majd az aktiv√°ci√≥s f√ºggv√©nnyel eld√∂nti, hogy ebb≈ël milyen er≈ëss√©g≈± jelet k√ºldj√∂n tov√°bb."*

Vagy egy kicsit technikaibb megfogalmaz√°sban:

*"A neuron egy line√°ris transzform√°ci√≥val √∂sszes√≠ti a bemeneteket, amelynek eredm√©ny√©t egy nem-line√°ris aktiv√°tor form√°lja v√©gleges kimenett√©."*


## Logit (s√∫lyozott √∂sszeg)

A logit (ejtsd: lodzsit) a g√©pi tanul√°sban a neuron nyers, m√©g nem normaliz√°lt kimenet√©t jelenti, k√∂zvetlen√ºl azel≈ëtt, hogy az aktiv√°ci√≥s f√ºggv√©ny (p√©ld√°ul a Softmax vagy a Sigmoid) √°talak√≠tan√° azt val√≥sz√≠n≈±s√©gg√©.

A logit matematikailag nem m√°s, mint a neuron √°ltal v√©grehajtott line√°ris regresszi√≥ k√∂zvetlen eredm√©nye.

Ez azt jelenti, hogy a logit kisz√°m√≠t√°sa minden esetben ‚Äì az aktiv√°l√°s el≈ëtt ‚Äì szigor√∫an az al√°bbi line√°ris k√©plettel t√∂rt√©nik:


$$
logit =  \sum_{i=1}^{n} w_i x_i + b )
$$


### 2. Honnan j√∂n a n√©v? (A matematikai h√°tt√©r)
A kifejez√©s a statisztik√°b√≥l sz√°rmazik, √©s a "log-odds" (logaritmikus es√©ly) r√∂vid√≠t√©se.

Ha adott egy 
$$p$$

val√≥sz√≠n≈±s√©g (ahol $0<p<1 $), akkor az "es√©ly" (odds) a bek√∂vetkez√©s √©s a be nem k√∂vetkez√©s ar√°nya:
$$
Odds=\frac{p}{1-p}
$$
‚Äã
 
A logit f√ºggv√©ny ennek a term√©szetes logaritmusa 


$$
logit(p) = ln (\frac{p}{1-p})
$$


<br>

### Mi√©rt nem haszn√°lunk m√°sodfok√∫ egyenletet a neuron belsej√©ben?

$(ax^2+bx+c)$ 

Elm√©letileg lehetne olyan neur√°lis h√°l√≥t √©p√≠teni, ahol a neuronok m√°sodfok√∫ f√ºggv√©nyeket sz√°molnak (l√©teznek is √∫gynevezett "Higher-order Neural Networks" vagy "Polynomial Neural Networks"), de a gyakorlatban nem ezek terjedtek el. Ennek h√°rom f≈ë oka van:
* Univerzalit√°s (Universal Approximation Theorem): A matematikusok bebizony√≠tott√°k, hogy ha sok egyszer≈±, line√°ris neuronb√≥l √°ll√≥ r√©teget egym√°s ut√°n pakolunk, √©s k√∂z√©j√ºk rakunk nem-line√°ris aktiv√°ci√≥s f√ºggv√©nyeket, akkor ez a h√°l√≥zat b√°rmilyen bonyolult f√ºggv√©nyt (ak√°r huszadfok√∫t vagy szinuszosat is) k√©pes lemodellezni. Nem kell bonyolult neuron, ha sok egyszer≈± neuron egy√ºtt k√©pes a bonyolult viselked√©sre.
* Sz√°m√≠t√°si k√∂lts√©g: A line√°ris m≈±velet (w‚ãÖx) sz√°m√≠t√≥g√©pes szempontb√≥l nagyon olcs√≥ (m√°trixszorz√°s, amit a vide√≥k√°rty√°k im√°dnak). Ha minden egyes neuronban elkezden√©nk n√©gyzetre emelni, szorzatokat k√©pezni $(x_1*x_2)$, az exponenci√°lisan megn√∂veln√© a tan√≠t√°shoz sz√ºks√©ges sz√°m√≠t√°si id≈ët.
* T√∫lilleszt√©s (Overfitting): Egy m√°sodfok√∫ vagy magasabb fok√∫ polinom sokkal "hajl√©konyabb", mint egy egyenes. Ha minden neuron alapb√≥l ilyen bonyolult lenne, a h√°l√≥zat hajlamos lenne "bemagolni" az adatokat ahelyett, hogy megtanuln√° az √∂sszef√ºgg√©seket. A line√°ris neuronok "but√°bbak", ez√©rt stabilabban tanulnak, ha sokat tesz√ºnk bel≈ël√ºk egym√°s ut√°n.



## Aktiv√°ci√≥s f√ºggv√©nyek

Az aktiv√°ci√≥s f√ºggv√©nyek a neur√°lis h√°l√≥k "lelke", ezek teszik lehet≈ëv√©, hogy a h√°l√≥zat ne csak egyenes vonalakat (line√°ris kapcsolatokat) tanuljon meg, hanem bonyolult, g√∂rb√ºlt √∂sszef√ºgg√©seket is.

* Sigmoid
* Tanh
* ReLU
* Softmax



**Mi√©rt h√≠vjuk "aktiv√°ci√≥nak"?**: A biol√≥giai anal√≥gia miatt. Az agyunkban a neuronok nem folytonosan k√ºldenek jeleket. Csak akkor "t√ºzelnek" (aktiv√°l√≥dnak), ha az ingerek (a bemenetek √∂sszege) el√©rnek egy bizonyos k√ºsz√∂b√∂t.

Ha az inger gyenge ‚Üí A neuron csendben marad (Kimenet
$$
‚âà0
$$

Ha az inger √°tl√©pi a k√ºsz√∂b√∂t ‚Üí A neuron "els√ºl", elektromos jelet k√ºld tov√°bb (Kimenet
$$
>0
$$


N√©zz√ºk sorban a legfontosabbakat, amikkel 99%-ban tal√°lkozni fogsz.


### Sigmoid (S-g√∂rbe)
Ez a klasszikus aktiv√°ci√≥s f√ºggv√©ny, amelyet sok√°ig alap√©rtelmezettk√©nt haszn√°ltak.

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

 
![](docs/image-2025-12-13-22-52-35.png)
 
Tulajdons√°gai:

* Kimeneti tartom√°ny: 0 √©s 1 k√∂z√∂tt. 
* M≈±k√∂d√©se: A bemeneti √©rt√©keket ($z$) egy 0 √©s 1 k√∂z√∂tti val√≥sz√≠n≈±s√©gi √©rt√©kre nyomja √∂ssze. A nagyon negat√≠v sz√°mokb√≥l $0$, a nagyon pozit√≠vakb√≥l $1$ lesz. 
*Felhaszn√°l√°sa: F≈ëleg a kimeneti r√©tegen, bin√°ris oszt√°lyoz√°sn√°l (pl. "beteg" vagy "eg√©szs√©ges"), mert az eredm√©ny val√≥sz√≠n≈±s√©gk√©nt √©rtelmezhet≈ë.
* H√°tr√°nya: A "gradiens elt≈±n√©se" probl√©ma. A g√∂rbe k√©t sz√©l√©n a deriv√°lt majdnem nulla, ami miatt a tan√≠t√°s (a s√∫lyok friss√≠t√©se) nagyon lelassulhat m√©ly h√°l√≥zatokn√°l.


### Tanh (Hiperbolikus Tangens)
A Sigmoidhoz hasonl√≥ S-alak√∫ g√∂rbe, de matematikailag el≈ëny√∂sebb tulajdons√°gokkal rendelkezik a rejtett r√©tegekben.


$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$

‚Äã![](docs/image-2025-12-13-22-52-57.png)
 
Tulajdons√°gai:
* Kimeneti tartom√°ny: $-1$ √©s $1$ k√∂z√∂tt. 
* El≈ënye: Az √°tlaga 0 (zero-centered). Ez az√©rt fontos, mert ha a neuron kimenete pozit√≠v √©s negat√≠v is lehet, az stabilabb√° teszi a k√∂vetkez≈ë r√©teg tanul√°s√°t, szemben a Sigmoiddal, ami mindig pozit√≠v ir√°nyba tolja az adatokat.


### ReLU (Rectified Linear Unit)
A modern m√©lytanul√°s (Deep Learning) leggyakrabban haszn√°lt f√ºggv√©nye. B√°r egyszer≈±nek t≈±nik, ez teszi lehet≈ëv√© a nagyon m√©ly h√°l√≥k hat√©kony tan√≠t√°s√°t.

A k√©plet LaTeX k√≥dja:

$$
f(z) = \max(0, z)
$$


![](docs/image-2025-12-13-22-53-17.png)

Ez matematikailag k√©t √°gb√≥l √°ll:

* Ha $z<0$, az eredm√©ny 0
* Ha $z‚â•0$, az eredm√©ny maga a $z$


Tulajdons√°gai:
* Kimeneti tartom√°ny: $0$ √©s $+\infty$ k√∂z√∂tt.
* El≈ënye: Sz√°m√≠t√°si ig√©nye rendk√≠v√ºl kicsi (nem kell hatv√°nyozni), √©s a pozit√≠v tartom√°nyban nem t≈±nik el a gradiens (a deriv√°lt mindig 1), √≠gy a hiba√ºzenet akad√°lytalanul jut vissza a h√°l√≥zat elej√©re.
* H√°tr√°nya ("Dying ReLU"): Ha a bemenet negat√≠v, a neuron "kikapcsol" √©s a deriv√°ltja 0 lesz. Ha egy neuron mindig negat√≠v bemenetet kap, soha t√∂bb√© nem tud tanulni.

### Softmax (A d√∂nt√©shoz√≥)
Ez egy speci√°lis f√ºggv√©ny, amit szinte kiz√°r√≥lag a h√°l√≥zat legutols√≥ r√©teg√©ben haszn√°lunk, ha t√∂bb lehets√©ges kateg√≥ria k√∂z√ºl kell v√°lasztani. Az 1 val√≥sz√≠n≈±s√©get sz√©tosztja a lehets√©ges kimenetek k√∂z√∂tt. 

$$
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

 
Egy olyan val√≥sz√≠n≈±s√©gi eloszl√°st kapunk, ahol a kimenetek √∂sszege pontosan 1 (vagyis 100%). 

P√©ld√°ul: 
* Kutya: 0.7
* Macska: 0.2
* L√≥: 0.1

---
# Neuron h√°l√≥zatok 

A legegyszer≈±bb neur√°lis modellben egyetlen bemeneti √©rt√©ket kapunk, √©s egyetlen ‚Äòneuront‚Äô haszn√°lunk, ami a $ùë¶=ùë§ùë• + ùëè$ (esetleg egy aktiv√°ci√≥s f√ºggv√©nnyel kieg√©sz√≠tve). Ilyen modellek l√©teznek √©s hasznosak (pl. line√°ris/logisztikus regresszi√≥), de a legt√∂bb val√≥s, √∂sszetett feladathoz √∂nmagukban nem elegend≈ëk. A gyakorlatban a neuronokat p√°rhuzamosan futtatjuk (ebb≈ël lesz egy r√©teg), √©s sok ilyen r√©teget f≈±z√ºnk egym√°s ut√°n (ebb≈ël lesz a ‚Äòdeep‚Äô h√°l√≥). A bemenet tipikusan t√∂bb jellemz≈ëb≈ël √°ll, √©s a kimenet lehet egyetlen √©rt√©k vagy ak√°r t√∂bb dimenzi√≥s vektor is. Most menj√ºnk v√©gig l√©p√©sr≈ël l√©p√©sre a legegyszer≈±bb modellt≈ël a val√≥di neur√°lis h√°l√≥zatokig


## Egy neuronos h√°l√≥: 1 bemenet ->  1 kimenet

Egy 1 bemenettel rendelkez≈ë, 1 neuronos h√°l√≥zat a neur√°lis h√°l√≥k legegyszer≈±bb, m√©gis teljes √©rt√©k≈± alapegys√©ge. Ebben a modellben egyetlen numerikus bemeneti √©rt√©ket adunk a rendszernek, amelyet a neuron el≈ësz√∂r egy line√°ris m≈±velettel feldolgoz: a bemenetet megszorozza egy tanulhat√≥ s√∫llyal, majd hozz√°ad egy szint√©n tanulhat√≥ eltol√°st (bias). Ez a l√©p√©s matematikailag a 
$$z=wx+b$$ 
k√©plettel √≠rhat√≥ le, √©s ezt az √©rt√©ket nevezz√ºk el≈ëaktiv√°ci√≥nak. √ñnmag√°ban ez m√©g nem t√∂bb, mint egy egyszer≈± egyenes az input‚Äìoutput t√©rben.

Ahhoz azonban, hogy a modell ne csak line√°ris √∂sszef√ºgg√©seket tudjon le√≠rni, a neuron egy m√°sodik l√©p√©sben alkalmaz egy aktiv√°ci√≥s f√ºggv√©nyt az el≈ëaktiv√°ci√≥ra. Ez az aktiv√°ci√≥s f√ºggv√©ny egy fix, nem tanulhat√≥ matematikai f√ºggv√©ny (pl. sigmoid vagy ReLU), amely meghat√°rozza, hogyan alakul √°t a line√°ris kimenet v√©gs≈ë neuronkimenett√©. A teljes neuron m≈±k√∂d√©s√©t √≠gy a 
$$y=œÉ(wx+b)$$
k√©plet √≠rja le. Az aktiv√°ci√≥s f√ºggv√©ny szerepe az, hogy nemlinearit√°st vigyen a modellbe, vagyis megt√∂rje azt a korl√°tot, hogy a kimenet mindig csak egyenes ment√©n v√°ltozhasson.

![](docs/image-2025-12-14-19-26-30.png)

Ha az aktiv√°ci√≥s f√ºggv√©ny az identit√°s f√ºggv√©ny, akkor ez a neuron pontosan egy line√°ris regresszi√≥s modellnek felel meg, amely csak egyenes kapcsolatokat k√©pes tanulni. Ha sigmoid aktiv√°ci√≥t haszn√°lunk, akkor a kimenet 0 √©s 1 k√∂z√© esik, √©s a modell logisztikus regresszi√≥k√©nt viselkedik, vagyis bin√°ris d√∂nt√©sekhez alkalmas. M√°s aktiv√°ci√≥k, p√©ld√°ul a ReLU, m√°r t√∂r√©spontot hoznak l√©tre a f√ºggv√©nyben, ami fontos √©p√≠t≈ëk√∂ve a m√©ly neur√°lis h√°l√≥k expresszivit√°s√°nak. A tan√≠t√°s sor√°n kiz√°r√≥lag a s√∫ly √©s a bias √©rt√©ke v√°ltozik, az aktiv√°ci√≥s f√ºggv√©ny alakja nem.

Fontos meg√©rteni, hogy b√°r ez a modell rendk√≠v√ºl egyszer≈±, koncepcion√°lisan ugyanazokat az elveket haszn√°lja, mint a t√∂bb r√©tegb≈ël √©s t√∂bb ezer neuront tartalmaz√≥ m√©ly h√°l√≥k. Egyetlen neuron azonban csak nagyon korl√°tozott mint√°zatokat tud megtanulni, ez√©rt a gyakorlatban a neuronokat p√°rhuzamosan r√©tegekbe szervezz√ºk, majd ezeket a r√©tegeket egym√°s ut√°n kapcsoljuk. Minden bonyolult neur√°lis h√°l√≥ v√©gs≈ë soron ilyen egyszer≈±, 1 bemenet≈± neuronok matematikai



## Egy neuronos h√°l√≥: t√∂bb bemenet -> 1 kimenet

√Åltal√°ban egy neuron nem egyetlen bemenetet, hanem t√∂bb bemeneti jellemz≈ët kap, mik√∂zben tov√°bbra is egyetlen kimeneti √©rt√©ket √°ll√≠t el≈ë. Ez a helyzet m√°r sokkal k√∂zelebb √°ll a val√≥s probl√©m√°khoz, hiszen a legt√∂bb feladatban egy d√∂nt√©st vagy becsl√©st t√∂bb szempont, t√∂bb m√©rhet≈ë tulajdons√°g alapj√°n kell meghozni. Matematikailag a bemenet ilyenkor nem egy skal√°r, hanem egy vektor, a neuron pedig minden egyes bemeneti komponenshez k√ºl√∂n s√∫lyt rendel.


![](docs/image-2025-12-14-19-52-25.png)

A neuron m≈±k√∂d√©se ebben az esetben sem v√°ltozik elvileg: a bemeneti vektort egy s√∫lyvektorral szorozzuk √∂ssze, majd hozz√°adunk egy bias √©rt√©ket, √≠gy kapjuk meg az el≈ëaktiv√°ci√≥t. Ez a m≈±velet kompakt form√°ban a 

$$z=w^Tx+b$$
 
kifejez√©ssel √≠rhat√≥ le, amely nem m√°s, mint a kor√°bban megismert egyv√°ltoz√≥s k√©plet vektoros √°ltal√°nos√≠t√°sa. Az el≈ëaktiv√°ci√≥ra ezut√°n ugyan√∫gy egy aktiv√°ci√≥s f√ºggv√©nyt alkalmazunk, amely meghat√°rozza a neuron v√©gs≈ë kimenet√©t. A bemenetek sz√°m√°t√≥l f√ºggetlen√ºl egy neuronnak mindig 1 eltol√°sa van. 


Fontos hangs√∫lyozni, hogy ebben a modellben tov√°bbra is csak egyetlen neuronr√≥l besz√©l√ºnk: nincs p√°rhuzamos feldolgoz√°s √©s nincs r√©tegstrukt√∫ra. A neuron egyetlen skal√°r kimenetet ad vissza, amely a bemeneti t√©rben egy line√°ris d√∂nt√©si fel√ºletet (egyenest, s√≠kot vagy hipers√≠kot) reprezent√°l. Ez a fel√©p√≠t√©s m√°r alkalmas egyszer≈±bb regresszi√≥s √©s klasszifik√°ci√≥s feladatokra, ugyanakkor j√≥l mutatja a modell korl√°tait is, √©s term√©szetes √°tvezet√©st ad a k√∂vetkez≈ë l√©p√©shez, ahol t√∂bb neuront kapcsolunk √∂ssze egyetlen r√©tegg√©.

<br>

## T√∂bb p√°rhuzamos neuronb√≥l -> neuron r√©teg

A neur√°lis h√°l√≥zatok alapvet≈ë √©p√≠t≈ëk√∂ve a neuron r√©teg (layer), amely sz√°mos, egym√°s mellett p√°rhuzamosan m≈±k√∂d≈ë mesters√©ges neuronb√≥l √°ll. Ezen a szinten a sz√°m√≠t√°si folyamatok rendk√≠v√ºl hat√©konyan optimaliz√°lhat√≥k, mivel a r√©teget alkot√≥ neuronok bels≈ë line√°ris m≈±veletei ‚Äì a s√∫lyozott √∂sszegz√©sek ‚Äì egym√°st√≥l f√ºggetlenek, √≠gy szimult√°n v√©grehajthat√≥k. 

![](docs/image-2025-12-14-22-54-26.png)

A szerkezetet a 'teljes √∂sszek√∂t√∂tts√©g' (fully connected) jellemzi: a bemeneti vektor (

$$x$$
) minden egyes elem√©t a r√©teg √∂sszes neuronja megkapja bemenetk√©nt, √©s saj√°t s√∫lyparam√©tereik (

$$w$$
) seg√≠ts√©g√©vel dolgozz√°k fel azt. B√°r minden neuron ugyanazt az inform√°ci√≥halmazt l√°tja, az elt√©r≈ë s√∫lyoz√°s miatt mindegyik√ºk m√°s-m√°s jellemz≈ët emel ki bel≈ële, √©s v√©g√ºl minden egyes neuron pontosan egyetlen skal√°r √©rt√©ket gener√°l. √çgy a r√©teg √∂sszegzett kimenete ezen egyedi skal√°rokb√≥l √∂ssze√°ll√≥ √∫j vektor lesz, amelyet a rendszer matematikailag egyetlen l√©p√©sben, m√°trixm≈±veletek seg√≠ts√©g√©vel √°ll√≠t el≈ë

<br>

Minden egyes neuron a r√©tegben ugyanazt a bemeneti vektort (

$$x$$
) kapja meg.

De minden neuronnak saj√°t s√∫lyai (

$$w$$
) √©s saj√°t eltol√°sa (

$$b$$
) van.

Ez√©rt minden neuron egy kicsit m√°st "l√°t meg" ugyanabban a bemenetben.



**Matematikai le√≠r√°s:** Mikor t√∂bb p√°rhuzamosan m≈±k√∂d≈ë neuront haszn√°lunk, itt v√°lik a matematika skal√°rokb√≥l (egyetlen sz√°mokb√≥l) m√°trix-m≈±velett√©:

Ha egy r√©tegben

$$m$$
darab neuron van (a p√©ld√°ban $m=4$), √©s a bemenet dimenzi√≥ja

$$n$$

A p√©ld√°ban $n=3$


A sok kis s√∫lyvektort egym√°s al√° √≠rjuk, √≠gy kapunk egy m√°trixot. A m√°trix minden sora egy-egy neuron saj√°t s√∫lyait tartalmazza.

$$W‚ààR m√ón$$
 
A biasok (

$$b$$
) Minden neuronnak van egy saj√°t bias √©rt√©ke, ezeket egy vektorba rendezz√ºk.

$$b‚ààR m$$
 
Nem kell $m$ x kisz√°molni a k√©pletet. A m√°trixszorz√°s elv√©gzi ezt egyszerre az √∂sszes neuronra:

$$z=Wx+b$$




A fenti p√©lda alapj√°n: 

$$
W =
\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} \\
w_{2,1} & w_{2,2} & w_{2,3} \\
w_{3,1} & w_{3,2} & w_{3,3} \\
w_{4,1} & w_{4,2} & w_{4,3}
\end{bmatrix}
$$

A r√©teg kimenet√©t (az aktiv√°ci√≥ el≈ëtti $y$ √©rt√©keket) az al√°bbi m√°trixszorz√°ssal sz√°moljuk ki. Itt j√≥l l√°tszik, hogyan tal√°lkozik a 4 neuron a 3 bemenettel:

$$
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
y_4
\end{bmatrix}
=

\begin{bmatrix}
w_{1,1} & w_{1,2} & w_{1,3} \\
w_{2,1} & w_{2,2} & w_{2,3} \\
w_{3,1} & w_{3,2} & w_{3,3} \\
w_{4,1} & w_{4,2} & w_{4,3}
\end{bmatrix}
\cdot
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4
\end{bmatrix}
$$


A v√©gs≈ë kimenetet az aktiv√°ci√≥s f√ºggv√©ny alkalmaz√°s√°val kapjuk meg: 

$$
\begin{bmatrix}
z_1 \\
z_2 \\
z_3 \\
z_4   
\end{bmatrix}
=

\sigma\left(
\begin{bmatrix}
 y_1 \\
y_2 \\
y_3 \\
y_4   
\end{bmatrix}
\right)


$$


### M√°trix szorz√°s kifejtve:  



$$ (Wx)_1 = w_{1,1}x_1 + w_{1,2}x_2 + w_{1,3}x_3 $$


$$ (Wx)_2 = w_{2,1}x_1 + w_{2,2}x_2 + w_{2,3}x_3 $$

$$ (Wx)_3 = w_{3,1}x_1 + w_{3,2}x_2 + w_{3,3}x_3 $$

$$ (Wx)_4 = w_{4,1}x_1 + w_{4,2}x_2 + w_{4,3}x_3 $$

Bias hozz√°ad√°sa elemenk√©nt:

$$ y_1 = (Wx)_1 + b_1 $$
$$ y_2 = (Wx)_2 + b_2 $$
$$ y_3 = (Wx)_3 + b_3 $$
$$ y_4 = (Wx)_4 + b_4 $$

Teljesen kifejtett alak (minden egy sorban):

$$ y_1 = w_{1,1}x_1 + w_{1,2}x_2 + w_{1,3}x_3 + b_1 $$
$$ y_2 = w_{2,1}x_1 + w_{2,2}x_2 + w_{2,3}x_3 + b_2 $$
$$ y_3 = w_{3,1}x_1 + w_{3,2}x_2 + w_{3,3}x_3 + b_3 $$
$$ y_4 = w_{4,1}x_1 + w_{4,2}x_2 + w_{4,3}x_3 + b_4 $$

√âs itt minden $y_i$-re alkalmazni kell az aktiv√°ci√≥s f√ºggv√©nyt $\sigma(y_i)$ 

√ñsszefoglal√≥, √°ltal√°nos alak (egy r√©teg matematikai defin√≠ci√≥ja):

$$
y_i = \sum_{j=1}^{3} w_{i,j} x_j + b_i
\qquad \text{ahol } i=1,2,3,4
$$

<br>

## T√∂bb neuron r√©tegb≈ël -> m√©ly neuron h√°l√≥zatok

$$
f_{\theta}(x) = \sigma(W_2 \, \sigma(W_1 x + b_1) + b_2)
$$



A t√∂bb neuronr√©tegb≈ël √°ll√≥ neur√°lis h√°l√≥zatok a kor√°bban bemutatott neuron-r√©teg fogalmat √°ltal√°nos√≠tj√°k √∫gy, hogy nem egyetlen r√©teget, hanem t√∂bb egym√°s ut√°n kapcsolt r√©teget alkalmazunk. Minden r√©teg egy √∂n√°ll√≥ transzform√°ci√≥t hajt v√©gre a bemeneti adaton: a bemenetet megszorozza egy s√∫lym√°trixszal, hozz√°ad egy bias vektort, majd egy aktiv√°ci√≥s f√ºggv√©nyen vezeti √°t. Az √≠gy kapott kimenet m√°r nem a modell v√©gs≈ë v√°lasza, hanem a k√∂vetkez≈ë r√©teg bemenete, vagyis az inform√°ci√≥ r√©tegr≈ël r√©tegre halad el≈ëre a h√°l√≥zatban.

Matematikailag ez azt jelenti, hogy az els≈ë r√©teg a nyers bemenetb≈ël egy √∫j reprezent√°ci√≥t hoz l√©tre, a k√∂vetkez≈ë r√©teg ezt a reprezent√°ci√≥t tov√°bb alak√≠tja, √©s √≠gy tov√°bb. Egy l-edik r√©tegben az el≈ëz≈ë r√©teg aktiv√°ci√≥j√°t tekintj√ºk bemenetnek, amelyre ism√©t egy line√°ris transzform√°ci√≥t √©s egy nemlinearit√°st alkalmazunk. Ennek form√°lis alakja a k√∂vetkez≈ë:

$$
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$

$$ a^{(l)} = \sigma^{(l)}\!\left(z^{(l)}\right) $$

Itt az $a^{(l‚àí1)}$ az el≈ëz≈ë r√©teg kimenete (aktiv√°ci√≥ja), a $W^l$ √©s $b^l$ az aktu√°lis r√©teg tanulhat√≥ param√©terei, m√≠g $\sigma^l$ az adott r√©teghez v√°lasztott aktiv√°ci√≥s f√ºggv√©ny. A h√°l√≥zat bemenet√©t gyakran az $a^{(0)}=x$ el√∂l√©ssel adjuk meg, jelezve, hogy a nyers input is egy ‚Äû0. r√©tegk√©nt‚Äù √©rtelmezhet≈ë.

![](docs/image-2025-12-14-22-53-53.png)

A t√∂bb r√©teg egym√°sra f≈±z√©se az√©rt kulcsfontoss√°g√∫, mert az aktiv√°ci√≥s f√ºggv√©nyek miatt a teljes h√°l√≥zat m√°r nem √≠rhat√≥ le egyetlen line√°ris transzform√°ci√≥k√©nt. Minden √∫j r√©teg tov√°bb n√∂veli a modell kifejez≈ëerej√©t, √©s lehet≈ëv√© teszi, hogy a h√°l√≥zat egyre √∂sszetettebb, hierarchikus mint√°zatokat tanuljon meg. A teljes h√°l√≥zat m≈±k√∂d√©se √≠gy egy egym√°sba √°gyazott f√ºggv√©nykompoz√≠ci√≥k√©nt foghat√≥ fel:

$$
f(x) = a^{(L)} = \sigma^{(L)}\left(
W^{(L)} ,\sigma^{(L-1)}!\left(
\cdots \sigma^{(1)}\left(W^{(1)}x + b^{(1)}\right)
\cdots \right)

b^{(L)}
\right)
$$
